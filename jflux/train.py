"""
Training script for Flux model using DiffusionDB dataset.

This implements flow matching training where:
- z_t = (1 - t) * noise + t * z_clean
- velocity v = z_clean - noise
- Loss = MSE(v_pred, v)

DiffusionDB is a large-scale text-to-image prompt dataset with 14M images 
generated by Stable Diffusion. See: https://github.com/poloclub/diffusiondb

Usage:
    # Train from scratch (default) with 1000 samples
    uv run jflux-train --num_samples 1000 --batch_size 4
    
    # Train from scratch with smaller T5 (saves disk space)
    uv run jflux-train --num_samples 1000 --t5_size base
    
    # Fine-tune from pretrained weights instead
    uv run jflux-train --num_samples 1000 --from_scratch False --learning_rate 1e-5
"""

import math
import os
from typing import Any, Iterator

import jax
import jax.numpy as jnp
import numpy as np
import optax
from chex import Array, PRNGKey
from einops import rearrange, repeat
from fire import Fire
from flax import nnx
from PIL import Image
from tqdm import tqdm

from jflux.model import Flux, FluxParams
from jflux.modules.autoencoder import AutoEncoder
from jflux.modules.conditioner import HFEmbedder
from jflux.modules.layers import timestep_embedding
from jflux.sampling import denoise, get_noise, get_schedule, prepare, unpack
from jflux.util import configs, get_t5_dim, load_ae, load_clip, load_flow_model, load_t5, torch2jax, MODEL_SCALES


os.environ["XLA_PYTHON_CLIENT_PREALLOCATE"] = "false"


def preprocess_image(
    image: Image.Image, target_size: int = 256
) -> np.ndarray:
    """Preprocess image to target size and normalize to [-1, 1]."""
    # Resize maintaining aspect ratio and center crop
    w, h = image.size
    scale = target_size / min(w, h)
    new_w, new_h = int(w * scale), int(h * scale)
    image = image.resize((new_w, new_h), Image.LANCZOS)
    
    # Center crop
    left = (new_w - target_size) // 2
    top = (new_h - target_size) // 2
    image = image.crop((left, top, left + target_size, top + target_size))
    
    # Convert to RGB if needed
    if image.mode != "RGB":
        image = image.convert("RGB")
    
    # Normalize to [-1, 1]
    arr = np.array(image, dtype=np.float32) / 127.5 - 1.0
    # HWC -> CHW
    arr = np.transpose(arr, (2, 0, 1))
    return arr


class DiffusionDBDataLoader:
    """
    DataLoader for DiffusionDB dataset.
    
    Downloads metadata.parquet and fetches images directly from HuggingFace.
    This avoids the deprecated dataset loading scripts.
    """
    
    # HuggingFace base URL for DiffusionDB zip files
    HF_BASE_URL = "https://huggingface.co/datasets/poloclub/diffusiondb/resolve/main/images"
    
    def __init__(
        self,
        num_samples: int = 1000,
        batch_size: int = 4,
        image_size: int = 256,
        shuffle: bool = True,
        seed: int = 42,
        cache_dir: str = "/mnt/diffusiondb_cache",
        filter_nsfw: bool = True,
        nsfw_threshold: float = 0.5,
    ):
        """
        Initialize DiffusionDB dataloader.
        
        Downloads ZIP files from HuggingFace containing images.
        Prompts come from metadata.parquet.
        
        Args:
            num_samples: Number of samples to use
            batch_size: Batch size for training
            image_size: Target image size (must be multiple of 16)
            shuffle: Whether to shuffle data
            seed: Random seed for reproducibility
            cache_dir: Directory to cache data
            filter_nsfw: Whether to filter NSFW images (uses metadata.parquet)
            nsfw_threshold: NSFW score threshold (0-1, images above filtered)
        """
        import zipfile
        import pandas as pd
        import requests
        from pathlib import Path
        from urllib.request import urlretrieve
        
        self.batch_size = batch_size
        self.image_size = image_size
        self.shuffle = shuffle
        self.rng = np.random.default_rng(seed)
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"Loading DiffusionDB dataset...")
        print(f"  Requested samples: {num_samples}")
        
        # Download metadata.parquet (contains prompts and NSFW scores)
        metadata_path = self.cache_dir / "metadata.parquet"
        if not metadata_path.exists():
            print(f"  Downloading metadata.parquet...")
            metadata_url = "https://huggingface.co/datasets/poloclub/diffusiondb/resolve/main/metadata.parquet"
            urlretrieve(metadata_url, metadata_path)
        
        # Load metadata
        print(f"  Loading metadata...")
        metadata_df = pd.read_parquet(metadata_path)
        print(f"    Total images in metadata: {len(metadata_df)}")
        
        # Filter NSFW if requested
        if filter_nsfw:
            original_len = len(metadata_df)
            metadata_df = metadata_df[
                (metadata_df["image_nsfw"] < nsfw_threshold) & 
                (metadata_df["prompt_nsfw"] < nsfw_threshold)
            ]
            print(f"    After NSFW filtering: {len(metadata_df)} images")
        
        # Select samples from consecutive parts to minimize downloads
        # Sort by part_id to group by ZIP file
        metadata_df = metadata_df.sort_values("part_id")
        
        # Take first N samples (from earliest parts)
        metadata_df = metadata_df.head(num_samples)
        
        # Then shuffle locally (for training randomization)
        if shuffle:
            metadata_df = metadata_df.sample(frac=1, random_state=seed)
        
        # Get unique part_ids we need to download
        part_ids = metadata_df["part_id"].unique()
        print(f"  Need to download {len(part_ids)} ZIP file(s) for {len(metadata_df)} samples")
        
        # Create images directory
        images_dir = self.cache_dir / "images"
        images_dir.mkdir(exist_ok=True)
        
        # Download and extract needed ZIP files
        for part_id in sorted(part_ids):
            part_name = f"part-{part_id:06d}"
            zip_path = self.cache_dir / f"{part_name}.zip"
            extract_dir = images_dir / part_name
            
            # Download ZIP if not cached
            if not zip_path.exists() and not extract_dir.exists():
                zip_url = f"{self.HF_BASE_URL}/{part_name}.zip"
                print(f"  Downloading {part_name}.zip...")
                try:
                    response = requests.get(zip_url, stream=True, timeout=300)
                    response.raise_for_status()
                    with open(zip_path, "wb") as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            f.write(chunk)
                except Exception as e:
                    print(f"    Warning: Failed to download {part_name}: {e}")
                    continue
            
            # Extract ZIP to part-specific directory
            if zip_path.exists() and not extract_dir.exists():
                print(f"  Extracting {part_name}.zip...")
                try:
                    extract_dir.mkdir(exist_ok=True)
                    with zipfile.ZipFile(zip_path, 'r') as zf:
                        zf.extractall(extract_dir)
                    # Remove ZIP after extraction to save space
                    zip_path.unlink()
                except Exception as e:
                    print(f"    Warning: Failed to extract {part_name}: {e}")
                    continue
        
        # Load images using metadata
        print(f"  Loading images...")
        self.samples = []
        for _, row in tqdm(metadata_df.iterrows(), total=len(metadata_df), desc="  Loading"):
            part_name = f"part-{row['part_id']:06d}"
            img_path = images_dir / part_name / row["image_name"]
            
            if img_path.exists():
                try:
                    img = Image.open(img_path)
                    self.samples.append({
                        "image": img.copy(),
                        "prompt": row["prompt"],
                    })
                    img.close()
                except Exception as e:
                    pass  # Skip failed images
        
        print(f"  Loaded {len(self.samples)} samples")
        
    def __len__(self) -> int:
        return len(self.samples) // self.batch_size
    
    def __iter__(self) -> Iterator[dict[str, Any]]:
        indices = np.arange(len(self.samples))
        if self.shuffle:
            self.rng.shuffle(indices)
        
        for i in range(0, len(indices) - self.batch_size + 1, self.batch_size):
            batch_indices = indices[i : i + self.batch_size]
            
            images = []
            prompts = []
            
            for idx in batch_indices:
                sample = self.samples[idx]
                img = sample["image"]
                
                if img is not None:
                    try:
                        img_arr = preprocess_image(img, self.image_size)
                        images.append(img_arr)
                        prompts.append(sample["prompt"])
                    except Exception as e:
                        print(f"  Warning: Failed to process image: {e}")
            
            if len(images) >= 1:
                yield {
                    "images": np.stack(images, axis=0),
                    "prompts": prompts,
                }


def create_img_ids(height: int, width: int, batch_size: int) -> Array:
    """Create image position IDs for the transformer."""
    h, w = height // 2, width // 2  # After patchifying
    img_ids = jnp.zeros((h, w, 3))
    img_ids = img_ids.at[..., 1].set(jnp.arange(h)[:, None])
    img_ids = img_ids.at[..., 2].set(jnp.arange(w)[None, :])
    img_ids = repeat(img_ids, "h w c -> b (h w) c", b=batch_size)
    return img_ids


def prepare_latents(
    ae: AutoEncoder,
    images: Array,
) -> Array:
    """Encode images to latent space using the VAE."""
    # images: (B, C, H, W) in [-1, 1]
    latents = ae.encode(images)
    return latents


def patchify(latents: Array) -> Array:
    """Convert latents to patch sequences for the transformer."""
    # latents: (B, C, H, W) -> (B, H*W/4, C*4)
    return rearrange(latents, "b c (h ph) (w pw) -> b (h w) (c ph pw)", ph=2, pw=2)


def unpatchify(patches: Array, height: int, width: int) -> Array:
    """Convert patch sequences back to latent images."""
    return rearrange(
        patches,
        "b (h w) (c ph pw) -> b c (h ph) (w pw)",
        h=height // 2 // 2,
        w=width // 2 // 2,
        ph=2,
        pw=2,
    )


def flow_matching_loss(
    model: Flux,
    latents: Array,  # Clean latents (B, seq, dim)
    img_ids: Array,
    txt: Array,
    txt_ids: Array,
    vec: Array,
    timesteps: Array,  # (B,)
    noise: Array,  # Same shape as latents
    guidance: Array | None = None,
) -> Array:
    """
    Compute flow matching loss.
    
    Flow matching interpolates between noise and data:
        z_t = (1 - t) * noise + t * z_clean
    
    The velocity is:
        v = z_clean - noise
    
    Loss is MSE between predicted and target velocity.
    """
    # Expand timesteps for broadcasting: (B,) -> (B, 1, 1)
    t = timesteps[:, None, None]
    
    # Interpolate: z_t = (1 - t) * noise + t * z_clean
    z_t = (1.0 - t) * noise + t * latents
    
    # Target velocity: v = z_clean - noise
    v_target = latents - noise
    
    # Predict velocity
    v_pred = model(
        img=z_t,
        img_ids=img_ids,
        txt=txt,
        txt_ids=txt_ids,
        y=vec,
        timesteps=timesteps,
        guidance=guidance,
    )
    
    # MSE loss
    loss = jnp.mean((v_pred - v_target) ** 2)
    return loss


def create_train_state(
    model: Flux,
    learning_rate: float,
    weight_decay: float,
    warmup_steps: int,
    total_steps: int,
    gradient_clip: float,
) -> tuple[nnx.Optimizer, optax.GradientTransformation]:
    """Create optimizer and learning rate schedule."""
    # Ensure decay_steps is at least 1
    decay_steps = max(1, total_steps - warmup_steps)
    
    # Learning rate schedule: linear warmup then cosine decay
    warmup_fn = optax.linear_schedule(
        init_value=0.0,
        end_value=learning_rate,
        transition_steps=max(1, warmup_steps),
    )
    decay_fn = optax.cosine_decay_schedule(
        init_value=learning_rate,
        decay_steps=decay_steps,
    )
    schedule_fn = optax.join_schedules(
        schedules=[warmup_fn, decay_fn],
        boundaries=[warmup_steps],
    )
    
    # Optimizer with gradient clipping
    tx = optax.chain(
        optax.clip_by_global_norm(gradient_clip),
        optax.adamw(learning_rate=schedule_fn, weight_decay=weight_decay),
    )
    
    optimizer = nnx.Optimizer(model, tx, wrt=nnx.Param)
    return optimizer, tx


@nnx.jit
def train_step(
    model: Flux,
    optimizer: nnx.Optimizer,
    img: Array,
    img_ids: Array,
    txt: Array,
    txt_ids: Array,
    vec: Array,
    timesteps: Array,
    noise: Array,
    guidance: Array | None,
) -> Array:
    """Single training step."""
    
    def loss_fn(model):
        return flow_matching_loss(
            model=model,
            latents=img,
            img_ids=img_ids,
            txt=txt,
            txt_ids=txt_ids,
            vec=vec,
            timesteps=timesteps,
            noise=noise,
            guidance=guidance,
        )
    
    loss, grads = nnx.value_and_grad(loss_fn)(model)
    optimizer.update(model, grads)  # Flax 0.11.0 requires (model, grads)
    return loss


def save_checkpoint(
    model: Flux,
    optimizer: nnx.Optimizer,
    step: int,
    output_dir: str,
):
    """Save model checkpoint."""
    os.makedirs(output_dir, exist_ok=True)
    path = os.path.join(output_dir, f"checkpoint_{step}")
    
    # Save model state
    state = nnx.state(model)
    # Use orbax or simple pickle for checkpointing
    # For simplicity, we'll use nnx's built-in serialization
    import pickle
    with open(f"{path}_model.pkl", "wb") as f:
        pickle.dump(state, f)
    
    print(f"Saved checkpoint to {path}")


def load_checkpoint(
    checkpoint_path: str,
    model_name: str = "flux-dev",
    model_scale: str = "small",
    t5_size: str = "base",
    device: str = "gpu",
) -> Flux:
    """
    Load a trained model from checkpoint.
    
    Args:
        checkpoint_path: Path to checkpoint file (e.g. "./checkpoints/checkpoint_1000_model.pkl")
        model_name: Model config name (flux-dev or flux-schnell)
        model_scale: Model scale used during training (tiny, small, base, full)
        t5_size: T5 encoder size used during training
        device: Device to load on (gpu, tpu, cpu)
        
    Returns:
        Loaded Flux model
    """
    import pickle
    
    # Get T5 dimension for context_in_dim
    t5_dim = get_t5_dim(t5_size)
    
    # Create model with same architecture
    model = load_flow_model(
        model_name,
        device=device,
        from_scratch=True,
        context_in_dim=t5_dim,
        model_scale=model_scale,
    )
    
    # Load saved state
    with open(checkpoint_path, "rb") as f:
        saved_state = pickle.load(f)
    
    # Update model with saved state
    nnx.update(model, saved_state)
    print(f"Loaded checkpoint from {checkpoint_path}")
    
    return model


def inference(
    checkpoint_path: str,
    prompt: str = "a beautiful sunset over mountains",
    # Model config (must match training)
    model_name: str = "flux-dev",
    model_scale: str = "small",
    t5_size: str = "base",
    # Generation settings
    width: int = 256,
    height: int = 256,
    num_steps: int = 50,
    guidance: float = 4.0,
    seed: int = 42,
    # Output
    output_path: str = "./generated.png",
):
    """
    Generate an image using a trained checkpoint.
    
    Args:
        checkpoint_path: Path to checkpoint file (e.g. "./checkpoints/checkpoint_1000_model.pkl")
        prompt: Text prompt for image generation
        model_name: Model config name (must match training)
        model_scale: Model scale (must match training)
        t5_size: T5 encoder size (must match training)
        width: Output image width (multiple of 16)
        height: Output image height (multiple of 16)
        num_steps: Number of denoising steps
        guidance: Guidance scale (only for flux-dev)
        seed: Random seed
        output_path: Where to save the generated image
        
    Example:
        uv run python -m jflux.train inference \\
            --checkpoint_path ./checkpoints/checkpoint_1000_model.pkl \\
            --prompt "a cat sitting on a windowsill" \\
            --model_scale small --t5_size base
    """
    # Detect device
    devices = jax.devices()
    device_info = devices[0]
    device_type = device_info.platform.upper()
    jax_device = device_type.lower()
    torch_device = "cuda" if jax_device == "gpu" else "cpu"
    
    print("=" * 60)
    print("Flux Inference")
    print("=" * 60)
    print(f"Checkpoint: {checkpoint_path}")
    print(f"Model scale: {model_scale}")
    print(f"Prompt: {prompt}")
    print(f"Size: {width}x{height}")
    print("=" * 60)
    
    # Ensure dimensions are valid
    width = 16 * (width // 16)
    height = 16 * (height // 16)
    
    # Load text encoders
    print("\nLoading text encoders...")
    t5 = load_t5(device=torch_device, max_length=512, model_size=t5_size)
    clip = load_clip(device=torch_device)
    
    # Load VAE
    print("Loading VAE...")
    ae = load_ae(model_name, device=jax_device)
    
    # Load trained model
    print("Loading trained model...")
    model = load_checkpoint(
        checkpoint_path=checkpoint_path,
        model_name=model_name,
        model_scale=model_scale,
        t5_size=t5_size,
        device=jax_device,
    )
    
    # Prepare inputs
    print("\nGenerating image...")
    key = jax.random.PRNGKey(seed)
    
    # Prepare initial noise and conditioning
    inp = prepare(
        t5=t5,
        clip=clip,
        img=get_noise(
            num_samples=1,
            height=height,
            width=width,
            dtype=jnp.bfloat16,
            seed=key,
        ),
        prompt=prompt,
    )
    x = inp["img"]
    img_ids = inp["img_ids"]
    txt = inp["txt"]
    txt_ids = inp["txt_ids"]
    vec = inp["vec"]
    
    # Get timestep schedule
    timesteps = get_schedule(
        num_steps=num_steps,
        image_seq_len=x.shape[1],
        shift=(model_name != "flux-schnell"),
    )
    
    # Denoise
    use_guidance = model_name == "flux-dev"
    x = denoise(
        model=model,
        img=x,
        img_ids=img_ids,
        txt=txt,
        txt_ids=txt_ids,
        vec=vec,
        timesteps=timesteps,
        guidance=guidance if use_guidance else None,
    )
    
    # Decode to image
    x = unpack(x, height=height, width=width)
    x = ae.decode(x)
    
    # Convert to PIL and save
    x = x[0]  # Remove batch dim
    x = jnp.clip((x + 1.0) / 2.0, 0.0, 1.0)  # [-1,1] -> [0,1]
    x = np.array(x)
    x = rearrange(x, "c h w -> h w c")
    x = (x * 255).astype(np.uint8)
    
    img = Image.fromarray(x)
    img.save(output_path)
    print(f"\nSaved to {output_path}")


def train(
    # Dataset
    num_samples: int = 10000,
    image_size: int = 256,
    cache_dir: str = "./diffusiondb_cache",
    filter_nsfw: bool = True,
    nsfw_threshold: float = 0.5,
    # Model
    model_name: str = "flux-dev",
    from_scratch: bool = True,
    t5_size: str = "base",
    model_scale: str = "base",  # "tiny", "small", "base", or "full"
    # Training
    num_epochs: int = 50,
    batch_size: int = 1,  # Flux is huge (~12B params), use small batch
    gradient_accumulation_steps: int = 4,  # Effective batch = batch_size * grad_accum
    learning_rate: float = 1e-4,
    weight_decay: float = 0.01,
    warmup_steps: int = 1000,
    gradient_clip: float = 1.0,
    # Checkpointing
    save_every: int = 1000,
    output_dir: str = "/mnt/checkpoints",  # Use /mnt for more space on cloud VMs
    # Logging
    log_every: int = 10,  # Print detailed metrics every N steps
    # Hardware
    seed: int = 42,
):
    """
    Train Flux model on DiffusionDB dataset.
    
    Args:
        num_samples: Number of samples from DiffusionDB to use (max 2M)
        image_size: Target image size (multiple of 16)
        cache_dir: Directory to cache downloaded metadata and images
        filter_nsfw: Whether to filter NSFW images/prompts
        nsfw_threshold: NSFW score threshold (0-1), images above are filtered
        model_name: "flux-dev" or "flux-schnell" (config for model architecture)
        from_scratch: If True, train from random init; if False, fine-tune from pretrained
        t5_size: T5 encoder size - "base" (250MB), "large" (800MB), "xl" (3GB), "xxl" (9.5GB)
        model_scale: Model size when training from scratch:
            - "tiny" (~50M params, ~2GB GPU) 
            - "small" (~400M params, ~8GB GPU)
            - "base" (~2B params, ~24GB GPU)
            - "full" (~12B params, ~96GB GPU)
        num_epochs: Number of training epochs
        batch_size: Batch size (use 1 for full Flux, may need more for smaller configs)
        gradient_accumulation_steps: Accumulate gradients over N steps (effective batch = batch_size * N)
        learning_rate: Peak learning rate (higher for from_scratch, e.g. 1e-4)
        weight_decay: AdamW weight decay
        warmup_steps: LR warmup steps
        gradient_clip: Max gradient norm
        save_every: Save checkpoint every N steps
        output_dir: Checkpoint directory
        log_every: Print detailed metrics every N steps
        seed: Random seed
        
    Note:
        Full Flux model is ~12B parameters and requires ~50GB+ GPU memory even with batch_size=1.
        For GPUs with less memory, use model_scale="small" (default) or "tiny".
    """
    # Detect device
    devices = jax.devices()
    device_info = devices[0]
    device_type = device_info.platform.upper()
    device_name = getattr(device_info, 'device_kind', device_type)
    
    mode = "FROM SCRATCH" if from_scratch else "FINE-TUNING"
    print("=" * 60)
    print(f"Flux Training Script - {mode}")
    print("=" * 60)
    print(f"Device: {device_name} ({device_type})")
    print(f"Number of devices: {len(devices)}")
    print(f"Dataset: DiffusionDB ({num_samples} samples)")
    print(f"Model: {model_name} ({'random init' if from_scratch else 'pretrained'})")
    if from_scratch:
        print(f"Model scale: {model_scale}")
    print(f"T5 encoder: {t5_size}")
    print(f"Image size: {image_size}x{image_size}")
    print(f"Batch size: {batch_size} (effective: {batch_size * gradient_accumulation_steps})")
    print(f"Learning rate: {learning_rate}")
    print(f"Epochs: {num_epochs}")
    print("=" * 60)
    
    # Set random seeds
    key = jax.random.PRNGKey(seed)
    np.random.seed(seed)
    
    # Ensure image size is valid
    assert image_size % 16 == 0, "Image size must be multiple of 16"
    
    # Load dataset
    print("\n[1/4] Loading DiffusionDB dataset...")
    dataloader = DiffusionDBDataLoader(
        num_samples=num_samples,
        batch_size=batch_size,
        image_size=image_size,
        shuffle=True,
        seed=seed,
        cache_dir=cache_dir,
        filter_nsfw=filter_nsfw,
        nsfw_threshold=nsfw_threshold,
    )
    
    # Calculate total steps
    steps_per_epoch = len(dataloader)
    if steps_per_epoch == 0:
        raise ValueError(
            f"Dataloader has no batches! Check that num_samples ({num_samples}) >= batch_size ({batch_size}) "
            f"and that images were successfully downloaded."
        )
    total_steps = steps_per_epoch * num_epochs
    print(f"Steps per epoch: {steps_per_epoch}")
    print(f"Total steps: {total_steps}")
    
    # Load models
    print("\n[2/4] Loading models...")
    # Use the detected device type
    jax_device = device_type.lower()  # "gpu", "tpu", or "cpu"
    torch_device = "cuda" if jax_device == "gpu" else "cpu"
    
    print(f"  JAX device: {jax_device}")
    print(f"  PyTorch device: {torch_device}")
    print("  Loading T5 encoder...")
    t5 = load_t5(device=torch_device, max_length=512, model_size=t5_size)
    t5_dim = get_t5_dim(t5_size)
    print(f"    T5 output dimension: {t5_dim}")
    print("  Loading CLIP encoder...")
    clip = load_clip(device=torch_device)
    print("  Loading VAE...")
    ae = load_ae(model_name, device=jax_device)
    print("  Initializing Flux model...")
    model = load_flow_model(
        model_name, 
        device=jax_device, 
        from_scratch=from_scratch,
        context_in_dim=t5_dim if from_scratch else None,
        model_scale=model_scale if from_scratch else "full",
    )
    
    # Pre-compute image IDs (same for all batches with same size)
    img_ids = create_img_ids(
        height=image_size // 8,  # VAE downsamples by 8
        width=image_size // 8,
        batch_size=batch_size,
    )
    
    # Create optimizer
    print("\n[3/4] Setting up optimizer...")
    # Ensure warmup_steps doesn't exceed total_steps
    effective_warmup = min(warmup_steps, total_steps // 10) if total_steps > 0 else 0
    print(f"  Warmup steps: {effective_warmup} (of {total_steps} total)")
    optimizer, _ = create_train_state(
        model=model,
        learning_rate=learning_rate,
        weight_decay=weight_decay,
        warmup_steps=effective_warmup,
        total_steps=total_steps,
        gradient_clip=gradient_clip,
    )
    
    # Training loop
    print("\n[4/4] Starting training...")
    global_step = 0
    use_guidance = model_name == "flux-dev"
    
    # Metrics tracking
    import time
    all_losses = []  # Track all losses for statistics
    training_start_time = time.time()
    
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        epoch_losses = []  # Track losses within epoch for variance
        num_batches = 0
        epoch_start_time = time.time()
        
        progress = tqdm(
            dataloader,
            desc=f"Epoch {epoch + 1}/{num_epochs}",
            total=len(dataloader),
        )
        
        for batch in progress:
            # Prepare data
            images = jnp.array(batch["images"])
            prompts = batch["prompts"]
            actual_batch_size = images.shape[0]
            
            # Skip if batch is too small (can happen with download failures)
            if actual_batch_size < 1:
                continue
            
            # Encode images to latents
            latents = prepare_latents(ae, images)
            latents_seq = patchify(latents)
            
            # Get text embeddings
            txt = torch2jax(t5(prompts))
            vec = torch2jax(clip(prompts))
            txt_ids = jnp.zeros((actual_batch_size, txt.shape[1], 3))
            
            # Recompute img_ids for actual batch size if different
            if actual_batch_size != batch_size:
                current_img_ids = create_img_ids(
                    height=image_size // 8,
                    width=image_size // 8,
                    batch_size=actual_batch_size,
                )
            else:
                current_img_ids = img_ids
            
            # Sample random timesteps
            key, subkey = jax.random.split(key)
            timesteps = jax.random.uniform(subkey, (actual_batch_size,), minval=0.0, maxval=1.0)
            
            # Sample noise
            key, subkey = jax.random.split(key)
            noise = jax.random.normal(subkey, latents_seq.shape, dtype=latents_seq.dtype)
            
            # Guidance (only for flux-dev)
            guidance = jnp.full((actual_batch_size,), 4.0) if use_guidance else None
            
            # Training step
            loss = train_step(
                model=model,
                optimizer=optimizer,
                img=latents_seq,
                img_ids=current_img_ids,
                txt=txt,
                txt_ids=txt_ids,
                vec=vec,
                timesteps=timesteps,
                noise=noise,
                guidance=guidance,
            )
            
            loss_val = float(loss)
            epoch_loss += loss_val
            epoch_losses.append(loss_val)
            all_losses.append(loss_val)
            num_batches += 1
            global_step += 1
            
            # Update progress bar with basic metrics
            progress.set_postfix({
                "loss": f"{loss_val:.4f}",
                "avg": f"{epoch_loss / num_batches:.4f}",
            })
            
            # Print detailed metrics every log_every steps
            if global_step % log_every == 0:
                elapsed = time.time() - training_start_time
                steps_per_sec = global_step / elapsed
                samples_per_sec = steps_per_sec * batch_size
                eta_seconds = (total_steps - global_step) / steps_per_sec if steps_per_sec > 0 else 0
                eta_hours = eta_seconds / 3600
                
                # Compute loss statistics
                recent_losses = all_losses[-100:] if len(all_losses) >= 100 else all_losses
                loss_mean = np.mean(recent_losses)
                loss_std = np.std(recent_losses)
                loss_min = min(recent_losses)
                loss_max = max(recent_losses)
                
                print(f"\n  [Step {global_step}/{total_steps}] "
                      f"Loss: {loss_val:.4f} | "
                      f"Avg(100): {loss_mean:.4f} +/- {loss_std:.4f} | "
                      f"Min/Max: {loss_min:.4f}/{loss_max:.4f}")
                print(f"    Speed: {steps_per_sec:.2f} steps/s, {samples_per_sec:.2f} samples/s | "
                      f"ETA: {eta_hours:.1f}h | "
                      f"Elapsed: {elapsed/3600:.1f}h")
            
            # Save checkpoint
            if global_step % save_every == 0:
                save_checkpoint(model, optimizer, global_step, output_dir)
        
        # End of epoch summary with detailed metrics
        epoch_time = time.time() - epoch_start_time
        avg_loss = epoch_loss / num_batches
        loss_std = np.std(epoch_losses) if len(epoch_losses) > 1 else 0.0
        
        print(f"\n{'='*60}")
        print(f"Epoch {epoch + 1}/{num_epochs} completed in {epoch_time/60:.1f} min")
        print(f"  Loss: {avg_loss:.4f} +/- {loss_std:.4f}")
        print(f"  Min/Max loss: {min(epoch_losses):.4f} / {max(epoch_losses):.4f}")
        print(f"  Batches: {num_batches}, Steps: {global_step}/{total_steps}")
        
        # Overall progress
        total_elapsed = time.time() - training_start_time
        overall_avg_loss = np.mean(all_losses)
        print(f"  Overall avg loss: {overall_avg_loss:.4f}")
        print(f"  Total time: {total_elapsed/3600:.2f}h")
        print(f"{'='*60}")
    
    # Save final checkpoint
    save_checkpoint(model, optimizer, global_step, output_dir)
    
    # Final training summary
    total_time = time.time() - training_start_time
    final_avg_loss = np.mean(all_losses)
    final_std = np.std(all_losses)
    first_losses = all_losses[:100] if len(all_losses) >= 100 else all_losses[:len(all_losses)//10+1]
    last_losses = all_losses[-100:] if len(all_losses) >= 100 else all_losses[-(len(all_losses)//10+1):]
    
    print("\n" + "=" * 60)
    print("TRAINING COMPLETED!")
    print("=" * 60)
    print(f"Total steps: {global_step}")
    print(f"Total time: {total_time/3600:.2f} hours")
    print(f"Average speed: {global_step / total_time:.2f} steps/s")
    print(f"\nLoss Statistics:")
    print(f"  Final avg loss: {final_avg_loss:.4f} +/- {final_std:.4f}")
    print(f"  First 100 steps avg: {np.mean(first_losses):.4f}")
    print(f"  Last 100 steps avg: {np.mean(last_losses):.4f}")
    print(f"  Improvement: {np.mean(first_losses) - np.mean(last_losses):.4f}")
    print(f"\nCheckpoint saved to: {output_dir}")
    print("=" * 60)


def app():
    """CLI entry point with train and inference commands."""
    Fire({
        "train": train,
        "inference": inference,
    })


if __name__ == "__main__":
    app()
