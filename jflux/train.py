"""
Training script for Flux model using DiffusionDB dataset.

This implements flow matching training where:
- z_t = (1 - t) * noise + t * z_clean
- velocity v = z_clean - noise
- Loss = MSE(v_pred, v)

DiffusionDB is a large-scale text-to-image prompt dataset with 14M images 
generated by Stable Diffusion. See: https://github.com/poloclub/diffusiondb

Usage:
    # Train from scratch (default) with 1000 samples
    uv run jflux-train --num_samples 1000 --batch_size 4
    
    # Train from scratch with smaller T5 (saves disk space)
    uv run jflux-train --num_samples 1000 --t5_size base
    
    # Fine-tune from pretrained weights instead
    uv run jflux-train --num_samples 1000 --from_scratch False --learning_rate 1e-5
"""

import math
import os
from typing import Any, Iterator

import jax
import jax.numpy as jnp
import numpy as np
import optax
from chex import Array, PRNGKey
from einops import rearrange, repeat
from fire import Fire
from flax import nnx
from PIL import Image
from tqdm import tqdm

from jflux.model import Flux, FluxParams
from jflux.modules.autoencoder import AutoEncoder
from jflux.modules.conditioner import HFEmbedder
from jflux.modules.layers import timestep_embedding
from jflux.util import configs, get_t5_dim, load_ae, load_clip, load_flow_model, load_t5, torch2jax


os.environ["XLA_PYTHON_CLIENT_PREALLOCATE"] = "false"


def preprocess_image(
    image: Image.Image, target_size: int = 256
) -> np.ndarray:
    """Preprocess image to target size and normalize to [-1, 1]."""
    # Resize maintaining aspect ratio and center crop
    w, h = image.size
    scale = target_size / min(w, h)
    new_w, new_h = int(w * scale), int(h * scale)
    image = image.resize((new_w, new_h), Image.LANCZOS)
    
    # Center crop
    left = (new_w - target_size) // 2
    top = (new_h - target_size) // 2
    image = image.crop((left, top, left + target_size, top + target_size))
    
    # Convert to RGB if needed
    if image.mode != "RGB":
        image = image.convert("RGB")
    
    # Normalize to [-1, 1]
    arr = np.array(image, dtype=np.float32) / 127.5 - 1.0
    # HWC -> CHW
    arr = np.transpose(arr, (2, 0, 1))
    return arr


class DiffusionDBDataLoader:
    """
    DataLoader for DiffusionDB dataset.
    
    Downloads metadata.parquet and fetches images directly from HuggingFace.
    This avoids the deprecated dataset loading scripts.
    """
    
    # HuggingFace base URL for DiffusionDB zip files
    HF_BASE_URL = "https://huggingface.co/datasets/poloclub/diffusiondb/resolve/main/images"
    
    def __init__(
        self,
        num_samples: int = 1000,
        batch_size: int = 4,
        image_size: int = 256,
        shuffle: bool = True,
        seed: int = 42,
        cache_dir: str = "/mnt/diffusiondb_cache",
        filter_nsfw: bool = True,
        nsfw_threshold: float = 0.5,
    ):
        """
        Initialize DiffusionDB dataloader.
        
        Downloads ZIP files from HuggingFace containing images.
        Prompts come from metadata.parquet.
        
        Args:
            num_samples: Number of samples to use
            batch_size: Batch size for training
            image_size: Target image size (must be multiple of 16)
            shuffle: Whether to shuffle data
            seed: Random seed for reproducibility
            cache_dir: Directory to cache data
            filter_nsfw: Whether to filter NSFW images (uses metadata.parquet)
            nsfw_threshold: NSFW score threshold (0-1, images above filtered)
        """
        import zipfile
        import pandas as pd
        import requests
        from pathlib import Path
        from urllib.request import urlretrieve
        
        self.batch_size = batch_size
        self.image_size = image_size
        self.shuffle = shuffle
        self.rng = np.random.default_rng(seed)
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"Loading DiffusionDB dataset...")
        print(f"  Requested samples: {num_samples}")
        
        # Download metadata.parquet (contains prompts and NSFW scores)
        metadata_path = self.cache_dir / "metadata.parquet"
        if not metadata_path.exists():
            print(f"  Downloading metadata.parquet...")
            metadata_url = "https://huggingface.co/datasets/poloclub/diffusiondb/resolve/main/metadata.parquet"
            urlretrieve(metadata_url, metadata_path)
        
        # Load metadata
        print(f"  Loading metadata...")
        metadata_df = pd.read_parquet(metadata_path)
        print(f"    Total images in metadata: {len(metadata_df)}")
        
        # Filter NSFW if requested
        if filter_nsfw:
            original_len = len(metadata_df)
            metadata_df = metadata_df[
                (metadata_df["image_nsfw"] < nsfw_threshold) & 
                (metadata_df["prompt_nsfw"] < nsfw_threshold)
            ]
            print(f"    After NSFW filtering: {len(metadata_df)} images")
        
        # Shuffle and select subset
        if shuffle:
            metadata_df = metadata_df.sample(n=min(num_samples, len(metadata_df)), random_state=seed)
        else:
            metadata_df = metadata_df.head(num_samples)
        
        # Get unique part_ids we need to download
        part_ids = metadata_df["part_id"].unique()
        print(f"  Need to download {len(part_ids)} ZIP file(s) for {len(metadata_df)} samples")
        
        # Create images directory
        images_dir = self.cache_dir / "images"
        images_dir.mkdir(exist_ok=True)
        
        # Download and extract needed ZIP files
        for part_id in sorted(part_ids):
            part_name = f"part-{part_id:06d}"
            zip_path = self.cache_dir / f"{part_name}.zip"
            extract_dir = images_dir / part_name
            
            # Download ZIP if not cached
            if not zip_path.exists() and not extract_dir.exists():
                zip_url = f"{self.HF_BASE_URL}/{part_name}.zip"
                print(f"  Downloading {part_name}.zip...")
                try:
                    response = requests.get(zip_url, stream=True, timeout=300)
                    response.raise_for_status()
                    with open(zip_path, "wb") as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            f.write(chunk)
                except Exception as e:
                    print(f"    Warning: Failed to download {part_name}: {e}")
                    continue
            
            # Extract ZIP to part-specific directory
            if zip_path.exists() and not extract_dir.exists():
                print(f"  Extracting {part_name}.zip...")
                try:
                    extract_dir.mkdir(exist_ok=True)
                    with zipfile.ZipFile(zip_path, 'r') as zf:
                        zf.extractall(extract_dir)
                    # Remove ZIP after extraction to save space
                    zip_path.unlink()
                except Exception as e:
                    print(f"    Warning: Failed to extract {part_name}: {e}")
                    continue
        
        # Load images using metadata
        print(f"  Loading images...")
        self.samples = []
        for _, row in tqdm(metadata_df.iterrows(), total=len(metadata_df), desc="  Loading"):
            part_name = f"part-{row['part_id']:06d}"
            img_path = images_dir / part_name / row["image_name"]
            
            if img_path.exists():
                try:
                    img = Image.open(img_path)
                    self.samples.append({
                        "image": img.copy(),
                        "prompt": row["prompt"],
                    })
                    img.close()
                except Exception as e:
                    pass  # Skip failed images
        
        print(f"  Loaded {len(self.samples)} samples")
        
    def __len__(self) -> int:
        return len(self.samples) // self.batch_size
    
    def __iter__(self) -> Iterator[dict[str, Any]]:
        indices = np.arange(len(self.samples))
        if self.shuffle:
            self.rng.shuffle(indices)
        
        for i in range(0, len(indices) - self.batch_size + 1, self.batch_size):
            batch_indices = indices[i : i + self.batch_size]
            
            images = []
            prompts = []
            
            for idx in batch_indices:
                sample = self.samples[idx]
                img = sample["image"]
                
                if img is not None:
                    try:
                        img_arr = preprocess_image(img, self.image_size)
                        images.append(img_arr)
                        prompts.append(sample["prompt"])
                    except Exception as e:
                        print(f"  Warning: Failed to process image: {e}")
            
            if len(images) >= 1:
                yield {
                    "images": np.stack(images, axis=0),
                    "prompts": prompts,
                }


def create_img_ids(height: int, width: int, batch_size: int) -> Array:
    """Create image position IDs for the transformer."""
    h, w = height // 2, width // 2  # After patchifying
    img_ids = jnp.zeros((h, w, 3))
    img_ids = img_ids.at[..., 1].set(jnp.arange(h)[:, None])
    img_ids = img_ids.at[..., 2].set(jnp.arange(w)[None, :])
    img_ids = repeat(img_ids, "h w c -> b (h w) c", b=batch_size)
    return img_ids


def prepare_latents(
    ae: AutoEncoder,
    images: Array,
) -> Array:
    """Encode images to latent space using the VAE."""
    # images: (B, C, H, W) in [-1, 1]
    latents = ae.encode(images)
    return latents


def patchify(latents: Array) -> Array:
    """Convert latents to patch sequences for the transformer."""
    # latents: (B, C, H, W) -> (B, H*W/4, C*4)
    return rearrange(latents, "b c (h ph) (w pw) -> b (h w) (c ph pw)", ph=2, pw=2)


def unpatchify(patches: Array, height: int, width: int) -> Array:
    """Convert patch sequences back to latent images."""
    return rearrange(
        patches,
        "b (h w) (c ph pw) -> b c (h ph) (w pw)",
        h=height // 2 // 2,
        w=width // 2 // 2,
        ph=2,
        pw=2,
    )


def flow_matching_loss(
    model: Flux,
    latents: Array,  # Clean latents (B, seq, dim)
    img_ids: Array,
    txt: Array,
    txt_ids: Array,
    vec: Array,
    timesteps: Array,  # (B,)
    noise: Array,  # Same shape as latents
    guidance: Array | None = None,
) -> Array:
    """
    Compute flow matching loss.
    
    Flow matching interpolates between noise and data:
        z_t = (1 - t) * noise + t * z_clean
    
    The velocity is:
        v = z_clean - noise
    
    Loss is MSE between predicted and target velocity.
    """
    # Expand timesteps for broadcasting: (B,) -> (B, 1, 1)
    t = timesteps[:, None, None]
    
    # Interpolate: z_t = (1 - t) * noise + t * z_clean
    z_t = (1.0 - t) * noise + t * latents
    
    # Target velocity: v = z_clean - noise
    v_target = latents - noise
    
    # Predict velocity
    v_pred = model(
        img=z_t,
        img_ids=img_ids,
        txt=txt,
        txt_ids=txt_ids,
        y=vec,
        timesteps=timesteps,
        guidance=guidance,
    )
    
    # MSE loss
    loss = jnp.mean((v_pred - v_target) ** 2)
    return loss


def create_train_state(
    model: Flux,
    learning_rate: float,
    weight_decay: float,
    warmup_steps: int,
    total_steps: int,
    gradient_clip: float,
) -> tuple[nnx.Optimizer, optax.GradientTransformation]:
    """Create optimizer and learning rate schedule."""
    # Ensure decay_steps is at least 1
    decay_steps = max(1, total_steps - warmup_steps)
    
    # Learning rate schedule: linear warmup then cosine decay
    warmup_fn = optax.linear_schedule(
        init_value=0.0,
        end_value=learning_rate,
        transition_steps=max(1, warmup_steps),
    )
    decay_fn = optax.cosine_decay_schedule(
        init_value=learning_rate,
        decay_steps=decay_steps,
    )
    schedule_fn = optax.join_schedules(
        schedules=[warmup_fn, decay_fn],
        boundaries=[warmup_steps],
    )
    
    # Optimizer with gradient clipping
    tx = optax.chain(
        optax.clip_by_global_norm(gradient_clip),
        optax.adamw(learning_rate=schedule_fn, weight_decay=weight_decay),
    )
    
    optimizer = nnx.Optimizer(model, tx, wrt=nnx.Param)
    return optimizer, tx


@nnx.jit
def train_step(
    model: Flux,
    optimizer: nnx.Optimizer,
    img: Array,
    img_ids: Array,
    txt: Array,
    txt_ids: Array,
    vec: Array,
    timesteps: Array,
    noise: Array,
    guidance: Array | None,
) -> Array:
    """Single training step."""
    
    def loss_fn(model):
        return flow_matching_loss(
            model=model,
            latents=img,
            img_ids=img_ids,
            txt=txt,
            txt_ids=txt_ids,
            vec=vec,
            timesteps=timesteps,
            noise=noise,
            guidance=guidance,
        )
    
    loss, grads = nnx.value_and_grad(loss_fn, wrt=nnx.Param)(model)
    optimizer.update(grads)
    return loss


def save_checkpoint(
    model: Flux,
    optimizer: nnx.Optimizer,
    step: int,
    output_dir: str,
):
    """Save model checkpoint."""
    os.makedirs(output_dir, exist_ok=True)
    path = os.path.join(output_dir, f"checkpoint_{step}")
    
    # Save model state
    state = nnx.state(model)
    # Use orbax or simple pickle for checkpointing
    # For simplicity, we'll use nnx's built-in serialization
    import pickle
    with open(f"{path}_model.pkl", "wb") as f:
        pickle.dump(state, f)
    
    print(f"Saved checkpoint to {path}")


def train(
    # Dataset
    num_samples: int = 1000,
    image_size: int = 256,
    cache_dir: str = "./diffusiondb_cache",
    filter_nsfw: bool = True,
    nsfw_threshold: float = 0.5,
    # Model
    model_name: str = "flux-dev",
    from_scratch: bool = True,
    t5_size: str = "base",
    # Training
    num_epochs: int = 10,
    batch_size: int = 4,
    learning_rate: float = 1e-4,
    weight_decay: float = 0.01,
    warmup_steps: int = 1000,
    gradient_clip: float = 1.0,
    # Checkpointing
    save_every: int = 1000,
    output_dir: str = "./checkpoints",
    # Hardware
    seed: int = 42,
):
    """
    Train Flux model on DiffusionDB dataset.
    
    Args:
        num_samples: Number of samples from DiffusionDB to use (max 2M)
        image_size: Target image size (multiple of 16)
        cache_dir: Directory to cache downloaded metadata and images
        filter_nsfw: Whether to filter NSFW images/prompts
        nsfw_threshold: NSFW score threshold (0-1), images above are filtered
        model_name: "flux-dev" or "flux-schnell" (config for model architecture)
        from_scratch: If True, train from random init; if False, fine-tune from pretrained
        t5_size: T5 encoder size - "base" (250MB), "large" (800MB), "xl" (3GB), "xxl" (9.5GB)
        num_epochs: Number of training epochs
        batch_size: Batch size (reduce for memory)
        learning_rate: Peak learning rate (higher for from_scratch, e.g. 1e-4)
        weight_decay: AdamW weight decay
        warmup_steps: LR warmup steps
        gradient_clip: Max gradient norm
        save_every: Save checkpoint every N steps
        output_dir: Checkpoint directory
        seed: Random seed
    """
    # Detect device
    devices = jax.devices()
    device_info = devices[0]
    device_type = device_info.platform.upper()
    device_name = getattr(device_info, 'device_kind', device_type)
    
    mode = "FROM SCRATCH" if from_scratch else "FINE-TUNING"
    print("=" * 60)
    print(f"Flux Training Script - {mode}")
    print("=" * 60)
    print(f"Device: {device_name} ({device_type})")
    print(f"Number of devices: {len(devices)}")
    print(f"Dataset: DiffusionDB ({num_samples} samples)")
    print(f"Model: {model_name} ({'random init' if from_scratch else 'pretrained'})")
    print(f"T5 encoder: {t5_size}")
    print(f"Image size: {image_size}x{image_size}")
    print(f"Batch size: {batch_size}")
    print(f"Learning rate: {learning_rate}")
    print(f"Epochs: {num_epochs}")
    print("=" * 60)
    
    # Set random seeds
    key = jax.random.PRNGKey(seed)
    np.random.seed(seed)
    
    # Ensure image size is valid
    assert image_size % 16 == 0, "Image size must be multiple of 16"
    
    # Load dataset
    print("\n[1/4] Loading DiffusionDB dataset...")
    dataloader = DiffusionDBDataLoader(
        num_samples=num_samples,
        batch_size=batch_size,
        image_size=image_size,
        shuffle=True,
        seed=seed,
        cache_dir=cache_dir,
        filter_nsfw=filter_nsfw,
        nsfw_threshold=nsfw_threshold,
    )
    
    # Calculate total steps
    steps_per_epoch = len(dataloader)
    if steps_per_epoch == 0:
        raise ValueError(
            f"Dataloader has no batches! Check that num_samples ({num_samples}) >= batch_size ({batch_size}) "
            f"and that images were successfully downloaded."
        )
    total_steps = steps_per_epoch * num_epochs
    print(f"Steps per epoch: {steps_per_epoch}")
    print(f"Total steps: {total_steps}")
    
    # Load models
    print("\n[2/4] Loading models...")
    # Use the detected device type
    jax_device = device_type.lower()  # "gpu", "tpu", or "cpu"
    torch_device = "cuda" if jax_device == "gpu" else "cpu"
    
    print(f"  JAX device: {jax_device}")
    print(f"  PyTorch device: {torch_device}")
    print("  Loading T5 encoder...")
    t5 = load_t5(device=torch_device, max_length=512, model_size=t5_size)
    t5_dim = get_t5_dim(t5_size)
    print(f"    T5 output dimension: {t5_dim}")
    print("  Loading CLIP encoder...")
    clip = load_clip(device=torch_device)
    print("  Loading VAE...")
    ae = load_ae(model_name, device=jax_device)
    print("  Initializing Flux model...")
    model = load_flow_model(
        model_name, 
        device=jax_device, 
        from_scratch=from_scratch,
        context_in_dim=t5_dim if from_scratch else None,
    )
    
    # Pre-compute image IDs (same for all batches with same size)
    img_ids = create_img_ids(
        height=image_size // 8,  # VAE downsamples by 8
        width=image_size // 8,
        batch_size=batch_size,
    )
    
    # Create optimizer
    print("\n[3/4] Setting up optimizer...")
    # Ensure warmup_steps doesn't exceed total_steps
    effective_warmup = min(warmup_steps, total_steps // 10) if total_steps > 0 else 0
    print(f"  Warmup steps: {effective_warmup} (of {total_steps} total)")
    optimizer, _ = create_train_state(
        model=model,
        learning_rate=learning_rate,
        weight_decay=weight_decay,
        warmup_steps=effective_warmup,
        total_steps=total_steps,
        gradient_clip=gradient_clip,
    )
    
    # Training loop
    print("\n[4/4] Starting training...")
    global_step = 0
    use_guidance = model_name == "flux-dev"
    
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        num_batches = 0
        
        progress = tqdm(
            dataloader,
            desc=f"Epoch {epoch + 1}/{num_epochs}",
            total=len(dataloader),
        )
        
        for batch in progress:
            # Prepare data
            images = jnp.array(batch["images"])
            prompts = batch["prompts"]
            actual_batch_size = images.shape[0]
            
            # Skip if batch is too small (can happen with download failures)
            if actual_batch_size < 1:
                continue
            
            # Encode images to latents
            latents = prepare_latents(ae, images)
            latents_seq = patchify(latents)
            
            # Get text embeddings
            txt = torch2jax(t5(prompts))
            vec = torch2jax(clip(prompts))
            txt_ids = jnp.zeros((actual_batch_size, txt.shape[1], 3))
            
            # Recompute img_ids for actual batch size if different
            if actual_batch_size != batch_size:
                current_img_ids = create_img_ids(
                    height=image_size // 8,
                    width=image_size // 8,
                    batch_size=actual_batch_size,
                )
            else:
                current_img_ids = img_ids
            
            # Sample random timesteps
            key, subkey = jax.random.split(key)
            timesteps = jax.random.uniform(subkey, (actual_batch_size,), minval=0.0, maxval=1.0)
            
            # Sample noise
            key, subkey = jax.random.split(key)
            noise = jax.random.normal(subkey, latents_seq.shape, dtype=latents_seq.dtype)
            
            # Guidance (only for flux-dev)
            guidance = jnp.full((actual_batch_size,), 4.0) if use_guidance else None
            
            # Training step
            loss = train_step(
                model=model,
                optimizer=optimizer,
                img=latents_seq,
                img_ids=current_img_ids,
                txt=txt,
                txt_ids=txt_ids,
                vec=vec,
                timesteps=timesteps,
                noise=noise,
                guidance=guidance,
            )
            
            epoch_loss += float(loss)
            num_batches += 1
            global_step += 1
            
            # Update progress bar
            progress.set_postfix({
                "loss": f"{float(loss):.4f}",
                "avg_loss": f"{epoch_loss / num_batches:.4f}",
            })
            
            # Save checkpoint
            if global_step % save_every == 0:
                save_checkpoint(model, optimizer, global_step, output_dir)
        
        # End of epoch summary
        avg_loss = epoch_loss / num_batches
        print(f"\nEpoch {epoch + 1} completed. Average loss: {avg_loss:.4f}")
    
    # Save final checkpoint
    save_checkpoint(model, optimizer, global_step, output_dir)
    print("\nTraining completed!")


def app():
    """CLI entry point."""
    Fire(train)


if __name__ == "__main__":
    app()
